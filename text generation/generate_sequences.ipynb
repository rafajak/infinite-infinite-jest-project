{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate_sequences.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtrdjtP-5oVw",
        "colab_type": "code",
        "outputId": "149b861c-6282-4dae-a7f4-6a323e883b7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "!pip3 install gpt-2-simple"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gpt-2-simple in /usr/local/lib/python3.6/dist-packages (0.7.1)\n",
            "Requirement already satisfied: toposort in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (1.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (1.17.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gpt-2-simple) (2.21.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gpt-2-simple) (2.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T62nNp1l5Zd_",
        "colab_type": "code",
        "outputId": "dbca38d6-f7f1-48a5-ba20-d793281a7ef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        }
      },
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "import gpt_2_simple as gpt2\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import string\n",
        "from functools import wraps\n",
        "from time import time\n",
        "#Mounts google drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2gKs6vf5qo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CHECKPOINT_DIR = \"/content/drive/My Drive/GPT-2/checkpoints/\"\n",
        "SAMPLES_DIR = \"/content/drive/My Drive/GPT-2/generated_outputs/\"\n",
        "DEBUG_MODE = False\n",
        "MODEL_NAME = \"124M\" if DEBUG_MODE else \"1558M\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpfUHx095r9c",
        "colab_type": "code",
        "outputId": "b8d18e07-1e79-42dc-eeaf-d6f9964cf8f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "source": [
        "!ls  \"/content/drive/My Drive/GPT-2/checkpoints\"\n",
        "# !mkdir \"checkpoint/run1/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Copy of checkpoint'\n",
            "'Copy of encoder.json'\n",
            "'Copy of events.out.tfevents.1579217689.2718339f40d8'\n",
            "'Copy of hparams.json'\n",
            "'Copy of model-1000.data-00000-of-00001'\n",
            "'Copy of model-1000.index'\n",
            "'Copy of model-1000.meta'\n",
            "'Copy of vocab.bpe'\n",
            " infinite-infinite-jest.ipynb\n",
            " run1\n",
            "'testing_text_generation_schemes1(1).ipynb'\n",
            " _testing_text_generation_schemes.ipynb\n",
            "'Untitled folder'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHMcFAOy5tmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config= {\n",
        "    \"length\" : None,\n",
        "    \"return_as_list\" : True,\n",
        "    \"include_prefix\": True,\n",
        "    \"checkpoint_dir\": CHECKPOINT_DIR,\n",
        "    # \"temperature\" : 1.,\n",
        "    # \"top_k\": 40,\n",
        "    # \"top_p\": 0.9\n",
        "\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-717qs35wZ-",
        "colab_type": "code",
        "outputId": "ac6ee9c4-0299-4225-b849-031f793cf52a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, \n",
        "               checkpoint_dir = CHECKPOINT_DIR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading checkpoint /content/drive/My Drive/GPT-2/checkpoints/run1/model-1000\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/GPT-2/checkpoints/run1/model-1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vdkEbQF5w8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "OUTPUT_FILEPATH = SAMPLES_DIR + \"generated_output_\" + MODEL_NAME + \".json\"\n",
        "\n",
        "def timing(f):\n",
        "    @wraps(f)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time()\n",
        "        result = f(*args, **kwargs)\n",
        "        end = time()\n",
        "        print('Elapsed time: {}'.format((end-start)*1000))\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "def remove_bad_patterns(text):\n",
        "    data = text\n",
        "\n",
        "    for ch in list(string.ascii_lowercase) + [\",\"]:\n",
        "        for n_spaces in range(0,5):\n",
        "                pattern_match = ch + \" \" * n_spaces + \"\\n\"\n",
        "                data = data.replace(pattern_match, ch + \" \")\n",
        "    \n",
        "    multiple_newlines = [\"\\n\"*i for i in range(10,3,-1)] + [\"\\n \\n\"]    \n",
        "    for k in multiple_newlines:\n",
        "        data = data.replace(k,\"\\n\")\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def make_init_prompt(OUTPUT_FILEPATH=OUTPUT_FILEPATH):\n",
        "    \n",
        "    _output_dict = dict()\n",
        "    _output_dict[\"id\"] = 1\n",
        "    _output_dict[\"prompt\"] = config[\"prefix\"]\n",
        "    _output_dict[\"body\"] = \"I am seated in an office, surrounded by heads and bodies. \\nMy posture is consciously congruent to the shape of my hard chair.\"\n",
        "    _output_dict[\"params\"] = {\n",
        "        \"temperature\": None,\n",
        "        \"top_k\": None\n",
        "    }\n",
        "    with open(OUTPUT_FILEPATH, mode='w') as f:\n",
        "        json.dump([_output_dict], f, ensure_ascii=False, indent=4)\n",
        "    return\n",
        "\n",
        "\n",
        "def get_last_sequence_id():\n",
        "    with open(OUTPUT_FILEPATH, mode='r') as f:\n",
        "        print(OUTPUT_FILEPATH)\n",
        "        f = json.load(f)\n",
        "        return len(f)\n",
        "\n",
        "\n",
        "def get_hiperparameters():\n",
        "    temperature = np.random.choice([0.9,0.95,1])\n",
        "    top_k = np.random.choice([30,40, 50, 60,80])\n",
        "    return temperature, top_k\n",
        "\n",
        "\n",
        "def set_prefix(chunk):\n",
        "\n",
        "    #Preventing infinite dialogues, etc\n",
        "    # if random.random() < 0.1:\n",
        "    #     return None\n",
        "    # else:\n",
        "    with open(OUTPUT_FILEPATH, mode='r') as f:\n",
        "        f = json.load(f)\n",
        "\n",
        "    prompt =  f[-1][\"body\"]\n",
        "\n",
        "    if len(f) == 1:\n",
        "        return prompt\n",
        "    else:\n",
        "        tokens = min((len(prompt) // chunk), 1023)\n",
        "        prompt = prompt[-tokens:]\n",
        "        return prompt\n",
        "\n",
        "def set_length():\n",
        "    if config[\"prefix\"]:\n",
        "            length = 1023 - len(config[\"prefix\"])\n",
        "    else:\n",
        "        length = 0\n",
        "    return length\n",
        "\n",
        "@timing\n",
        "def generate_long_output(num_seq_to_gen=5,\n",
        "                         config=config,\n",
        "                         param_combination={}):\n",
        "\n",
        "    if DEBUG_MODE or not os.path.isfile(OUTPUT_FILEPATH):\n",
        "        make_init_prompt(OUTPUT_FILEPATH)\n",
        "\n",
        "    current_id = get_last_sequence_id() + 1\n",
        "\n",
        "    for curr_id in range(current_id, current_id + num_seq_to_gen):\n",
        "        print(\"=\"*50)\n",
        "        print(f\"Sequence {curr_id-1}/{num_seq_to_gen}\")\n",
        "\n",
        "        chunk_of_last_seq_to_use_for_prompt = random.randint(5, 12)\n",
        "\n",
        "        config[\"prefix\"] = set_prefix(chunk_of_last_seq_to_use_for_prompt)\n",
        "        config[\"length\"] = set_length()\n",
        "\n",
        "        temperature, top_k = get_hiperparameters()\n",
        "        \n",
        "        generated_batch = []\n",
        "\n",
        "        if DEBUG_MODE:\n",
        "            generated_batch = str(\n",
        "                config[\"prefix\"] + ((str(curr_id)+\" \") * 100))\n",
        "        else:\n",
        "            print(f\"Temperature: {temperature}, Top_k: {top_k}\")\n",
        "            print(\"Generating the sequence...\")\n",
        "            generated_batch = gpt2.generate(\n",
        "                sess, **config, temperature=temperature, top_k=top_k)\n",
        "\n",
        "        # Exclude prefix from the generated sequence\n",
        "        if DEBUG_MODE:\n",
        "            generated_batch = generated_batch[len(config[\"prefix\"]):]\n",
        "        elif config[\"prefix\"]:\n",
        "            generated_batch = generated_batch[0][len(config[\"prefix\"]):]\n",
        "        else:\n",
        "            generated_batch = generated_batch[0]\n",
        "\n",
        "        generated_batch = remove_bad_patterns(generated_batch)\n",
        "        print(\"len(generated_batch): \", len(generated_batch))\n",
        "        print(\"generated batch: \",print(generated_batch))\n",
        "\n",
        "        _output_dict = {}\n",
        "        _output_dict[\"id\"] = curr_id\n",
        "        _output_dict[\"prompt\"] = config[\"prefix\"]\n",
        "        _output_dict[\"body\"] = generated_batch\n",
        "        _output_dict[\"params\"] = {\n",
        "            \"temperature\": float(temperature),\n",
        "            \"top_k\": int(top_k),\n",
        "            \"chunk_size\": chunk_of_last_seq_to_use_for_prompt,\n",
        "            \"length\": config[\"length\"]\n",
        "        }\n",
        "\n",
        "        with open(OUTPUT_FILEPATH, \"r\") as f:\n",
        "            outputs = json.load(f)\n",
        "            outputs.append(_output_dict)\n",
        "\n",
        "        with open(OUTPUT_FILEPATH, \"w\") as f:\n",
        "            json.dump(outputs, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMx2JQKn5zdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_long_output(num_seq_to_gen=10000)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}