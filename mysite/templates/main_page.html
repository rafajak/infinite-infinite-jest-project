<!DOCTYPE html lang="en">
<html>
<head>
  <meta charset="utf-8" />
  <title>Infinite Infinite Jest</title>
  <link rel="icon" type="image/png" href={{ url_for('static', filename='/resources/favicon.png') }}>
  <meta name="description"
    content="Infinite Infinite Jest is an AI-powered art project showcasing prose generated by GPT-2 fine-tuned on David Foster Wallace's Infinite Jest.">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" type="text/css" href={{ url_for('static', filename='inf_jest.css') }}>
  <link rel="stylesheet" type="text/css" href={{ url_for('static', filename=css) }}>
  <link rel="stylesheet" type="text/css" href={{ url_for('static', filename='fonts.css') }}>
  <script src={{ url_for('static', filename='scrollPolyfill.js') }}></script>
  <script src={{ url_for('static', filename='inf_jest.js') }}></script>
</head>

<body id="inf-jest" class="inf-jest">
  <div id="menu" class="menu menu--ij-book">
    <div id="menuLinkAbout" class="menu__link"><a class="nav-link nav-link-about" href="javascript:;"
        onclick="goToAbout()">What is it?</a></div>
    <div id="menuLinkBack" class="menu__link"><a class="nav-link nav-link-back" href="javascript:;"
        onclick="goToBook()">Back</a></div>
    <div id="menuAutoplay" class="menu__autoplay menu__autoplay--off">autoplay: </div>
  </div>
  <header id="header" class="header">
    <img id="headerImage" class="header__image"
      src={{ url_for('static',filename='resources/final_header_1.png')}} />
    <video id='headerPlayer' class='header__player' style="display:none" oncanplay="headerPngToMp4()" autoplay muted
      loop playsinline>
      <source src={{ url_for('static', filename='resources/final_header_1.mp4') }} type="video/mp4">
    </video>
  </header>

  <main id="main" class="main main--ij-book">
    <div id="ij-book" class="ij-book ij-book--on">
      <div id="playerBefore" class="player-before"></div>
      <div id="playerAfter" class="player-after"></div>
      <div id="ij-book-pages-before" class="ij-book-pages-before"></div>
      <div id="ij-book-pages" class="ij-book-pages">
      </div>
      <div id="ij-book-loading" class="ij-book-loading">
        <img src={{ url_for('static', filename='/resources/loading.gif') }} alt="Loading…" />
      </div>
      <div id="ij-book-pages-after" class="ij-book-pages-after"></div>
    </div>
    <div id="ij-about" class="ij-about ij-about--off">
      <div id="playerBefore2" class="player-before"></div>
      <div id="playerAfter2" class="player-after"></div>
      <div id="aboutText" class="ij-about-text">
        <h3>Wait, what is this?</h3>
        <p><em>Infinite Infinite Jest</em> is an AI-powered art project. The main page displays prose generated by a
          state-of-the-art machine learning model (Open AI's GPT-2) fine-tuned on <a
            href="https://www.goodreads.com/book/show/6759.Infinite_Jest"><em>Infinite Jest</em></a>, a novel by David
          Foster Wallace.</p>
        <p>Part of each sequence of several paragraphs generated by the model is fed back into it as the prompt for the
          next original chunk of machine-generated prose.&nbsp;</p>
        <p>The process repeats. The result is Entertainment.</p>
        <h3>Why?</h3>
        <p>I am fascinated by generative AI models, and a fan of David Foster Wallace's work.</p>
        <p>In February 2019, a non-profit AI research company, Open AI, ignited a media storm with their unusual decision
          not to release their most recent language model. The model was said to be able to generate text on any topic
          with previously unseen coherence. The decision not to release the model was made amid fears over weaponising
          it and flooding the internet with convincing fake news and propaganda. </p>
        <p>After reading the <a href="https://openai.com/blog/better-language-models/">initial statement</a> by Open AI,
          and seeing the examples of text written by GPT-2, I couldn&rsquo;t shake this visceral sense that something
          profound has or is about to change. The change wouldn&rsquo;t be limited to how effective swarms of social
          media bots can become; it extends to the wider context of seeing creative writing as an inherently human
          endeavour.</p>
        <p>I thought there's no better book than <em>Infinite Jest</em> to be fed into this neural network of ominous
          fame.&nbsp; The idea of a seemingly infinite machine-generated prose based on a book about losing battles to
          addiction and weaponised modern entertainment seemed poignant and worth exploring.&nbsp;</p>
        <p> After a couple of months, Open AI shared their plan for a staged release of GPT-2. The delay was meant to
          improve societal preparedness for the new quality in text generation. In November 2019, Open AI published the
          largest version of the model, which is the one used for generating <em>Infinite Infinite Jest</em>.</p>
        <p>I've read hundreds of pages of the generated text samples while babysitting the model. Prose generated by
          GPT-2-Wall-Es can be stubbornly repetitious, contradictory, and hardly coherent long-term. But it sure has its
          moments of wit, and true, airy Wallacean absurd.&nbsp;</p>
        <h3>Are there any <em>Infinite Jest </em>spoilers?</h3>
        <p>(tl;dr: No.) The generated text contains characters from the novel, as well as its major themes and
          locations, but other than the opening line used as the first prompt, I haven't found a single sentence taken
          directly from DFW. There shouldn&rsquo;t be anything that would end up spoiling the experience of reading the
          actual book. That said, I now realise I&rsquo;m not sure what an effective spoiler to <em>Infinite Jest</em>
          would look like.</p>
        <h3>How does it work, exactly?</h3>
        <p>The neural network model (GPT-2) is fine-tuned on <em>Infinite Jest</em>:</p>
        <h4>What is GPT-2?</h4>
        <p>GPT-2 is an<a href="https://en.wikipedia.org/wiki/Unsupervised_learning"> unsupervised</a> artificial neural
          network, a <a href="https://en.wikipedia.org/wiki/Generative_model">generative</a> <a
            href="https://en.wikipedia.org/wiki/Language_model">language model</a> developed by Open AI. The original
          model (available <a href="https://github.com/openai/gpt-2">here</a>) was trained on a dataset containing over
          8 million documents (40GB of text) shared in Reddit submissions with at least 3 upvotes.&nbsp;</p>
        <p>Various interactive examples (like <a href="https://talktotransformer.com/">Talk to Transformer</a>)
          demonstrate the model&rsquo;s ability to generate coherent sequences of text based on user-defined prompt. The
          apparent versatility of the model is unparalleled. Give it a news headline, and it will continue describing
          the event in a journalistic fashion; feed it a few verses from a poem, and it will try carrying it on. Ask it
          a question, and it will give you an answer: truthful or bogus, but usually coherent.&nbsp;</p>
        <p>These abilities refer to the generic model shared by Open AI.&nbsp; What happens, if we fine-tune it on a
          single book, orienting it more towards a specific novel&rsquo;s style and content?</p>
        <h4>What does it mean for a neural network to be fine-tuned?</h4>
        <p>In the broader context of machine learning, &ldquo;fine-tuning&rdquo; a model means taking a model trained to
          do one task to do another task. The process involves freezing most of the networks layers, and retraining the
          remaining layers of the model on a new dataset.</p>
        <p>In our case, fine-tuning GPT-2 should allow the model to retain the knowledge of high-level linguistic
          patterns (learnt on the 40GB Reddit dataset), while paying particular attention to patterns specific to the
          text it is fine-tuned to (ie. <em>Infinite Jest</em>). I was aiming to train the model as to learn the themes
          and characters from the book, and convincingly imitate the author's writing style, while not overfitting to
          the text, ie. not copying the sentences from the book directly.&nbsp;</p>
        <h4>Is it really infinite?</h4>
        <p>It&rsquo;s not, but so far it&rsquo;s already significantly longer than <a
            href="https://miro.medium.com/max/474/1*3xbePnUI6DcPEX_mAXbY7Q.jpeg">the original book</a>.</p>
        <h3>What quirks does the machine-generated <em>Infinite Infinite Jest</em> have?</h3>
        <ul>
          <li>For some reason the model picked on the monologues of Jim's (James Incandenza&rsquo;s) dad. I found it
            funny that many sentences in some of the rambling monologues that the model is spewing end with ",
            Jim.".&nbsp; It's been a while since I last read the book, but I don't think these parts comprised more
            than, say, 5% of the book. It's interesting that the model fixated on them so much. My pet theory is that
            the model sees a rambling semi-coherent monologue it's producing, and then looks back at it and thinks "Oh,
            I must be inside one of these dumb Jim's dad monologues"</li><br>
          <li>Text contains neologisms that I was sure were taken verbatim from the book, but were in fact original to
            the generated text.&nbsp;</li><br>

          <li>I found the phrase "to eat cheese" (frequently used in the book, meaning: to share something incriminating
            about somebody with the authorities) appearing numerous times in the text in a benign culinary context. Very
            cute.&nbsp;</li><br>
          <li>Word repetitions (sometimes the model fixates on a single word and uses it excessively in a single
            paragraph.) Repetition is certainly a feature of David Foster Wallace&rsquo;s style [citation needed], but
            GPT-2-Wall-Es is taking this too far sometimes.&nbsp;</li><br>
          <li>I&rsquo;ve seen long sequences of sentences beginning with &ldquo;That (...)&rdquo;, clearly inspired by
            similar segments from the source book (pages 200-205 in my copy) containing life-lessons from the rehab. Due
            to the feedback loop (of using part of the last sequence as the prompt for the next sequence), once the
            model starts generating lists of sentences starting with &ldquo;That&rdquo;, it&rsquo;s hard for it to
            escape it.</li><br>
          <li>Feeding the model numbered lists as a prompt resulted in an output resembling <em>Infinite
              Jest</em>&rsquo;s endnotes (mostly bogus descriptions of pharmaceuticals, etc).&nbsp;</li><br>
        </ul>
        <h3>Who are you?</h3>
        <p>Rafal Jakubanis (<a href="mailto:rafajak@gmail.com">rafajak@gmail.com</a>) - concept, machine learning,
          writing the accompanying article.</p>
        <p>Mateusz Zaręba (<a href="mailto:mateuszzareba@protonmail.com">mateuszzareba@protonmail.com</a>) - frontend
          engineering</p>
        <p>To the best of my knowledge, none of the authors are affiliated and/or associated with Quebecois separatist
          movements.</p>
        <h3>Special thanks to:</h3>
        <p>Max Woolf (@minimaxir) - for creating gpt-2-simple, a toolkit for fine-tuning GPT-2</p>
        <p>Google Cloud Platform - for making TPUs available for free via Google Colab&nbsp;</p>
        <p>Open AI -&nbsp; for hard work on advancing the field of artificial intelligence while setting precedents in
          responsible releases of transformative technologies.&nbsp;</p>
        <p>Anna Warso, Piotr Migdał (@pmigdal) - for guidance, proofreading the human-generated text above.</p>
      </div>
    </div>
  </main>

  <footer id="footer" class="footer footer--ij-book">
    <div id="footerMenu" class="footer__menu">
      <div id="menuLabel" class="footer__menu__label">Scroll toggle</div>
      <button id="playButton" class="footer__menu__play-button paused" aria-label="Toggle autoscroll"
        onclick="togglePlayButton(event, this, true)"></button>
      <input id="scrollSpeed" class="footer__menu__scroll-speed" oninput="scrollSpeedChange()" type="number" value="150"
        min="0" max="600" step="10" />
    </div>
    <div id="footerDisclaimer" class="footer__disclaimer">Disclaimer: The text is generated by GPT-2, an AI model.
      Authors of the site don't necessarily agree with the sentiments expressed in the machine-generated text.</div>
  </footer>
</body>

</html>